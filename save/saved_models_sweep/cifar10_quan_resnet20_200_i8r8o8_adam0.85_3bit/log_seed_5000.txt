save path : ./save/2019-11-22/cifar10_quan_resnet20_200_i8r8o8_adam0.85_3bit
{'AD_sigma': 0.0, 'DA_sigma': 0.0, 'arch': 'quan_resnet20', 'attack_sample_size': 128, 'batch_size': 128, 'data_path': './dataset/', 'dataset': 'cifar10', 'decay': 1e-06, 'enable_bfa': False, 'epochs': 200, 'evaluate': False, 'fine_tune': False, 'gammas': [0.1, 0.1, 0.5], 'gpu_id': 0, 'input_M2D': 0.85, 'input_grain_size': [1, 8], 'input_num_bits': 3, 'k_top': 10, 'learning_rate': 0.001, 'manualSeed': 5000, 'model_only': False, 'momentum': 0.9, 'n_iter': 20, 'ngpu': 1, 'optimize_step': False, 'optimizer': 'Adam', 'output_M2D': 0.85, 'output_grain_size': [1, 8], 'output_num_bits': 3, 'print_freq': 100, 'regular_factor': 0.0, 'res_M2D': 0.85, 'res_grain_size': [1, 8], 'res_num_bits': 3, 'reset_weight': False, 'resume': '', 'save_path': './save/2019-11-22/cifar10_quan_resnet20_200_i8r8o8_adam0.85_3bit', 'schedule': [80, 120, 160], 'start_epoch': 0, 'use_cuda': True, 'workers': 4}
Random Seed: 5000
python version : 3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31)  [GCC 7.3.0]
torch  version : 1.1.0
cudnn  version : 7600
=> creating model 'quan_resnet20'
=> network :
 CifarResNet(
  (conv_1_3x3): quan_Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): quan_Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): quan_Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): quan_Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): quan_Linear(in_features=64, out_features=10, bias=True)
)
=> do not use any checkpoint for quan_resnet20 model

==>>[2019-11-23 23:08:45] [Epoch=000/200] [Need: 00:00:00] [LR=0.0010][M=0.90] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 1.631 (1.631)   Data 0.166 (0.166)   Loss 4.6841 (4.6841)   Prec@1 12.500 (12.500)   Prec@5 49.219 (49.219)   [2019-11-23 23:08:47]
  Epoch: [000][100/391]   Time 0.054 (0.062)   Data 0.000 (0.002)   Loss 1.7372 (2.0925)   Prec@1 32.812 (25.046)   Prec@5 87.500 (76.988)   [2019-11-23 23:08:52]
  Epoch: [000][200/391]   Time 0.055 (0.056)   Data 0.000 (0.001)   Loss 1.7715 (1.8875)   Prec@1 35.156 (31.347)   Prec@5 89.844 (82.750)   [2019-11-23 23:08:57]
  Epoch: [000][300/391]   Time 0.042 (0.054)   Data 0.000 (0.001)   Loss 1.3834 (1.7577)   Prec@1 51.562 (35.932)   Prec@5 94.531 (85.652)   [2019-11-23 23:09:02]
  **Train** Prec@1 39.066 Prec@5 87.322 Error@1 60.934
  **Test** Prec@1 50.430 Prec@5 93.900 Error@1 49.570
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:09:08] [Epoch=001/200] [Need: 01:15:07] [LR=0.0010][M=0.90] [Best : Accuracy=50.43, Error=49.57]
  Epoch: [001][000/391]   Time 0.258 (0.258)   Data 0.186 (0.186)   Loss 1.2497 (1.2497)   Prec@1 54.688 (54.688)   Prec@5 92.969 (92.969)   [2019-11-23 23:09:09]
  Epoch: [001][100/391]   Time 0.072 (0.049)   Data 0.000 (0.002)   Loss 1.2800 (1.3081)   Prec@1 54.688 (52.576)   Prec@5 95.312 (94.059)   [2019-11-23 23:09:13]
  Epoch: [001][200/391]   Time 0.042 (0.049)   Data 0.000 (0.001)   Loss 1.2216 (1.2573)   Prec@1 54.688 (54.544)   Prec@5 92.969 (94.516)   [2019-11-23 23:09:18]
  Epoch: [001][300/391]   Time 0.044 (0.049)   Data 0.000 (0.001)   Loss 1.1188 (1.2308)   Prec@1 59.375 (55.767)   Prec@5 94.531 (94.804)   [2019-11-23 23:09:23]
  **Train** Prec@1 56.842 Prec@5 95.052 Error@1 43.158
  **Test** Prec@1 55.120 Prec@5 95.510 Error@1 44.880
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:09:29] [Epoch=002/200] [Need: 01:11:41] [LR=0.0010][M=0.90] [Best : Accuracy=55.12, Error=44.88]
  Epoch: [002][000/391]   Time 0.266 (0.266)   Data 0.190 (0.190)   Loss 1.1126 (1.1126)   Prec@1 67.188 (67.188)   Prec@5 95.312 (95.312)   [2019-11-23 23:09:29]
  Epoch: [002][100/391]   Time 0.050 (0.049)   Data 0.000 (0.002)   Loss 1.1890 (1.0734)   Prec@1 60.156 (61.757)   Prec@5 96.875 (96.287)   [2019-11-23 23:09:34]
  Epoch: [002][200/391]   Time 0.040 (0.051)   Data 0.000 (0.001)   Loss 1.1697 (1.0447)   Prec@1 56.250 (62.516)   Prec@5 96.094 (96.510)   [2019-11-23 23:09:39]
  Epoch: [002][300/391]   Time 0.077 (0.050)   Data 0.000 (0.001)   Loss 1.0809 (1.0328)   Prec@1 64.844 (63.141)   Prec@5 94.531 (96.532)   [2019-11-23 23:09:44]
  **Train** Prec@1 63.588 Prec@5 96.674 Error@1 36.412
  **Test** Prec@1 62.390 Prec@5 96.700 Error@1 37.610
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:09:51] [Epoch=003/200] [Need: 01:11:17] [LR=0.0010][M=0.90] [Best : Accuracy=62.39, Error=37.61]
  Epoch: [003][000/391]   Time 0.268 (0.268)   Data 0.186 (0.186)   Loss 1.1684 (1.1684)   Prec@1 57.031 (57.031)   Prec@5 94.531 (94.531)   [2019-11-23 23:09:51]
  Epoch: [003][100/391]   Time 0.042 (0.049)   Data 0.000 (0.002)   Loss 0.9472 (0.9353)   Prec@1 65.625 (66.631)   Prec@5 97.656 (97.285)   [2019-11-23 23:09:56]
  Epoch: [003][200/391]   Time 0.042 (0.049)   Data 0.000 (0.001)   Loss 0.9402 (0.9236)   Prec@1 63.281 (67.188)   Prec@5 98.438 (97.392)   [2019-11-23 23:10:01]
  Epoch: [003][300/391]   Time 0.042 (0.048)   Data 0.000 (0.001)   Loss 0.7290 (0.9194)   Prec@1 75.781 (67.351)   Prec@5 98.438 (97.415)   [2019-11-23 23:10:05]
  **Train** Prec@1 67.814 Prec@5 97.464 Error@1 32.186
  **Test** Prec@1 63.130 Prec@5 95.670 Error@1 36.870
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:10:12] [Epoch=004/200] [Need: 01:10:19] [LR=0.0010][M=0.90] [Best : Accuracy=63.13, Error=36.87]
  Epoch: [004][000/391]   Time 0.264 (0.264)   Data 0.194 (0.194)   Loss 0.8322 (0.8322)   Prec@1 71.094 (71.094)   Prec@5 99.219 (99.219)   [2019-11-23 23:10:12]
  Epoch: [004][100/391]   Time 0.042 (0.051)   Data 0.000 (0.002)   Loss 0.8014 (0.8450)   Prec@1 70.312 (70.026)   Prec@5 96.875 (97.633)   [2019-11-23 23:10:17]
  Epoch: [004][200/391]   Time 0.041 (0.048)   Data 0.000 (0.001)   Loss 0.7117 (0.8298)   Prec@1 75.781 (70.775)   Prec@5 100.000 (97.746)   [2019-11-23 23:10:22]
  Epoch: [004][300/391]   Time 0.054 (0.048)   Data 0.000 (0.001)   Loss 0.6674 (0.8194)   Prec@1 76.562 (71.060)   Prec@5 100.000 (97.846)   [2019-11-23 23:10:26]
  **Train** Prec@1 71.376 Prec@5 97.950 Error@1 28.624
  **Test** Prec@1 67.770 Prec@5 97.710 Error@1 32.230
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:10:33] [Epoch=005/200] [Need: 01:09:31] [LR=0.0010][M=0.90] [Best : Accuracy=67.77, Error=32.23]
  Epoch: [005][000/391]   Time 0.265 (0.265)   Data 0.203 (0.203)   Loss 0.7456 (0.7456)   Prec@1 75.000 (75.000)   Prec@5 98.438 (98.438)   [2019-11-23 23:10:33]
  Epoch: [005][100/391]   Time 0.053 (0.052)   Data 0.000 (0.002)   Loss 0.6651 (0.7513)   Prec@1 76.562 (73.499)   Prec@5 98.438 (98.515)   [2019-11-23 23:10:38]
  Epoch: [005][200/391]   Time 0.043 (0.049)   Data 0.000 (0.001)   Loss 0.8242 (0.7518)   Prec@1 74.219 (73.675)   Prec@5 98.438 (98.340)   [2019-11-23 23:10:43]
  Epoch: [005][300/391]   Time 0.043 (0.048)   Data 0.000 (0.001)   Loss 0.7772 (0.7482)   Prec@1 75.781 (73.739)   Prec@5 96.875 (98.360)   [2019-11-23 23:10:47]
  **Train** Prec@1 74.154 Prec@5 98.390 Error@1 25.846
  **Test** Prec@1 68.520 Prec@5 98.010 Error@1 31.480
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:10:54] [Epoch=006/200] [Need: 01:08:58] [LR=0.0010][M=0.90] [Best : Accuracy=68.52, Error=31.48]
  Epoch: [006][000/391]   Time 0.261 (0.261)   Data 0.191 (0.191)   Loss 0.6643 (0.6643)   Prec@1 74.219 (74.219)   Prec@5 99.219 (99.219)   [2019-11-23 23:10:54]
  Epoch: [006][100/391]   Time 0.046 (0.048)   Data 0.000 (0.002)   Loss 0.7091 (0.7063)   Prec@1 76.562 (75.634)   Prec@5 99.219 (98.569)   [2019-11-23 23:10:59]
  Epoch: [006][200/391]   Time 0.051 (0.049)   Data 0.000 (0.001)   Loss 0.7646 (0.6920)   Prec@1 71.875 (75.944)   Prec@5 99.219 (98.597)   [2019-11-23 23:11:04]
  Epoch: [006][300/391]   Time 0.049 (0.049)   Data 0.000 (0.001)   Loss 0.6938 (0.6897)   Prec@1 74.219 (75.968)   Prec@5 99.219 (98.528)   [2019-11-23 23:11:08]
  **Train** Prec@1 76.196 Prec@5 98.524 Error@1 23.804
  **Test** Prec@1 73.860 Prec@5 98.210 Error@1 26.140
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:11:15] [Epoch=007/200] [Need: 01:08:25] [LR=0.0010][M=0.90] [Best : Accuracy=73.86, Error=26.14]
  Epoch: [007][000/391]   Time 0.262 (0.262)   Data 0.203 (0.203)   Loss 0.7250 (0.7250)   Prec@1 77.344 (77.344)   Prec@5 96.094 (96.094)   [2019-11-23 23:11:15]
  Epoch: [007][100/391]   Time 0.052 (0.049)   Data 0.000 (0.002)   Loss 0.7833 (0.6462)   Prec@1 74.219 (77.437)   Prec@5 97.656 (98.639)   [2019-11-23 23:11:20]
  Epoch: [007][200/391]   Time 0.082 (0.048)   Data 0.000 (0.001)   Loss 0.7205 (0.6450)   Prec@1 71.875 (77.647)   Prec@5 98.438 (98.651)   [2019-11-23 23:11:24]
  Epoch: [007][300/391]   Time 0.041 (0.049)   Data 0.000 (0.001)   Loss 0.8412 (0.6436)   Prec@1 67.188 (77.642)   Prec@5 98.438 (98.687)   [2019-11-23 23:11:30]
  **Train** Prec@1 77.648 Prec@5 98.706 Error@1 22.352
  **Test** Prec@1 75.680 Prec@5 98.580 Error@1 24.320
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:11:36] [Epoch=008/200] [Need: 01:08:14] [LR=0.0010][M=0.90] [Best : Accuracy=75.68, Error=24.32]
  Epoch: [008][000/391]   Time 0.263 (0.263)   Data 0.205 (0.205)   Loss 0.5986 (0.5986)   Prec@1 77.344 (77.344)   Prec@5 99.219 (99.219)   [2019-11-23 23:11:37]
  Epoch: [008][100/391]   Time 0.043 (0.049)   Data 0.000 (0.002)   Loss 0.5026 (0.6207)   Prec@1 83.594 (78.419)   Prec@5 100.000 (98.832)   [2019-11-23 23:11:41]
  Epoch: [008][200/391]   Time 0.043 (0.048)   Data 0.000 (0.001)   Loss 0.5406 (0.6129)   Prec@1 79.688 (78.669)   Prec@5 100.000 (98.850)   [2019-11-23 23:11:46]
  Epoch: [008][300/391]   Time 0.049 (0.048)   Data 0.000 (0.001)   Loss 0.6259 (0.6027)   Prec@1 78.125 (79.002)   Prec@5 100.000 (98.879)   [2019-11-23 23:11:51]
  **Train** Prec@1 79.068 Prec@5 98.856 Error@1 20.932
  **Test** Prec@1 76.280 Prec@5 98.320 Error@1 23.720
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:11:57] [Epoch=009/200] [Need: 01:07:36] [LR=0.0010][M=0.90] [Best : Accuracy=76.28, Error=23.72]
  Epoch: [009][000/391]   Time 0.254 (0.254)   Data 0.180 (0.180)   Loss 0.8028 (0.8028)   Prec@1 73.438 (73.438)   Prec@5 97.656 (97.656)   [2019-11-23 23:11:57]
  Epoch: [009][100/391]   Time 0.053 (0.052)   Data 0.000 (0.002)   Loss 0.4932 (0.5667)   Prec@1 80.469 (80.105)   Prec@5 99.219 (99.041)   [2019-11-23 23:12:02]
  Epoch: [009][200/391]   Time 0.042 (0.052)   Data 0.000 (0.001)   Loss 0.5774 (0.5693)   Prec@1 82.031 (80.166)   Prec@5 97.656 (99.013)   [2019-11-23 23:12:07]
  Epoch: [009][300/391]   Time 0.044 (0.050)   Data 0.000 (0.001)   Loss 0.6592 (0.5673)   Prec@1 78.906 (80.222)   Prec@5 97.656 (98.985)   [2019-11-23 23:12:12]
  **Train** Prec@1 79.990 Prec@5 98.978 Error@1 20.010
  **Test** Prec@1 78.340 Prec@5 98.810 Error@1 21.660
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:12:18] [Epoch=010/200] [Need: 01:07:19] [LR=0.0010][M=0.90] [Best : Accuracy=78.34, Error=21.66]
  Epoch: [010][000/391]   Time 0.261 (0.261)   Data 0.206 (0.206)   Loss 0.5490 (0.5490)   Prec@1 79.688 (79.688)   Prec@5 100.000 (100.000)   [2019-11-23 23:12:19]
  Epoch: [010][100/391]   Time 0.050 (0.050)   Data 0.000 (0.002)   Loss 0.4094 (0.5496)   Prec@1 86.719 (81.057)   Prec@5 99.219 (99.141)   [2019-11-23 23:12:23]
  Epoch: [010][200/391]   Time 0.080 (0.050)   Data 0.000 (0.001)   Loss 0.4282 (0.5553)   Prec@1 83.594 (80.892)   Prec@5 99.219 (99.028)   [2019-11-23 23:12:28]
  Epoch: [010][300/391]   Time 0.058 (0.050)   Data 0.000 (0.001)   Loss 0.4112 (0.5541)   Prec@1 85.156 (80.944)   Prec@5 99.219 (99.032)   [2019-11-23 23:12:33]
  **Train** Prec@1 80.872 Prec@5 99.048 Error@1 19.128
  **Test** Prec@1 75.300 Prec@5 98.430 Error@1 24.700

==>>[2019-11-23 23:12:40] [Epoch=011/200] [Need: 01:07:02] [LR=0.0010][M=0.90] [Best : Accuracy=78.34, Error=21.66]
  Epoch: [011][000/391]   Time 0.272 (0.272)   Data 0.202 (0.202)   Loss 0.3948 (0.3948)   Prec@1 87.500 (87.500)   Prec@5 99.219 (99.219)   [2019-11-23 23:12:40]
  Epoch: [011][100/391]   Time 0.051 (0.050)   Data 0.000 (0.002)   Loss 0.5659 (0.5259)   Prec@1 74.219 (81.938)   Prec@5 100.000 (99.118)   [2019-11-23 23:12:45]
  Epoch: [011][200/391]   Time 0.046 (0.049)   Data 0.000 (0.001)   Loss 0.5730 (0.5304)   Prec@1 81.250 (81.713)   Prec@5 97.656 (99.149)   [2019-11-23 23:12:50]
  Epoch: [011][300/391]   Time 0.043 (0.049)   Data 0.000 (0.001)   Loss 0.4724 (0.5329)   Prec@1 83.594 (81.587)   Prec@5 98.438 (99.151)   [2019-11-23 23:12:55]
  **Train** Prec@1 81.416 Prec@5 99.122 Error@1 18.584
  **Test** Prec@1 78.840 Prec@5 98.730 Error@1 21.160
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:13:01] [Epoch=012/200] [Need: 01:06:37] [LR=0.0010][M=0.90] [Best : Accuracy=78.84, Error=21.16]
  Epoch: [012][000/391]   Time 0.263 (0.263)   Data 0.193 (0.193)   Loss 0.4983 (0.4983)   Prec@1 82.812 (82.812)   Prec@5 100.000 (100.000)   [2019-11-23 23:13:01]
  Epoch: [012][100/391]   Time 0.042 (0.050)   Data 0.000 (0.002)   Loss 0.6474 (0.5196)   Prec@1 80.469 (81.846)   Prec@5 96.875 (99.095)   [2019-11-23 23:13:06]
  Epoch: [012][200/391]   Time 0.051 (0.050)   Data 0.000 (0.001)   Loss 0.4713 (0.5153)   Prec@1 82.031 (82.113)   Prec@5 100.000 (99.110)   [2019-11-23 23:13:11]
  Epoch: [012][300/391]   Time 0.052 (0.050)   Data 0.000 (0.001)   Loss 0.3732 (0.5158)   Prec@1 85.156 (82.052)   Prec@5 100.000 (99.138)   [2019-11-23 23:13:16]
  **Train** Prec@1 82.062 Prec@5 99.142 Error@1 17.938
  **Test** Prec@1 77.480 Prec@5 98.620 Error@1 22.520

==>>[2019-11-23 23:13:22] [Epoch=013/200] [Need: 01:06:18] [LR=0.0010][M=0.90] [Best : Accuracy=78.84, Error=21.16]
  Epoch: [013][000/391]   Time 0.261 (0.261)   Data 0.199 (0.199)   Loss 0.4611 (0.4611)   Prec@1 81.250 (81.250)   Prec@5 99.219 (99.219)   [2019-11-23 23:13:23]
  Epoch: [013][100/391]   Time 0.053 (0.055)   Data 0.000 (0.002)   Loss 0.5887 (0.5100)   Prec@1 78.906 (82.441)   Prec@5 100.000 (99.226)   [2019-11-23 23:13:28]
  Epoch: [013][200/391]   Time 0.045 (0.051)   Data 0.000 (0.001)   Loss 0.5370 (0.5038)   Prec@1 84.375 (82.680)   Prec@5 97.656 (99.238)   [2019-11-23 23:13:33]
  Epoch: [013][300/391]   Time 0.054 (0.051)   Data 0.000 (0.001)   Loss 0.6292 (0.5067)   Prec@1 78.906 (82.511)   Prec@5 98.438 (99.214)   [2019-11-23 23:13:38]
  **Train** Prec@1 82.520 Prec@5 99.184 Error@1 17.480
  **Test** Prec@1 77.740 Prec@5 98.490 Error@1 22.260

==>>[2019-11-23 23:13:44] [Epoch=014/200] [Need: 01:06:06] [LR=0.0010][M=0.90] [Best : Accuracy=78.84, Error=21.16]
  Epoch: [014][000/391]   Time 0.270 (0.270)   Data 0.200 (0.200)   Loss 0.5442 (0.5442)   Prec@1 78.125 (78.125)   Prec@5 100.000 (100.000)   [2019-11-23 23:13:45]
  Epoch: [014][100/391]   Time 0.051 (0.051)   Data 0.000 (0.002)   Loss 0.5236 (0.4775)   Prec@1 79.688 (83.222)   Prec@5 100.000 (99.366)   [2019-11-23 23:13:49]
  Epoch: [014][200/391]   Time 0.042 (0.049)   Data 0.000 (0.001)   Loss 0.5650 (0.4869)   Prec@1 81.250 (82.863)   Prec@5 100.000 (99.320)   [2019-11-23 23:13:54]
  Epoch: [014][300/391]   Time 0.042 (0.048)   Data 0.000 (0.001)   Loss 0.5684 (0.4845)   Prec@1 76.562 (83.012)   Prec@5 99.219 (99.304)   [2019-11-23 23:13:59]
  **Train** Prec@1 83.166 Prec@5 99.306 Error@1 16.834
  **Test** Prec@1 77.750 Prec@5 98.700 Error@1 22.250

==>>[2019-11-23 23:14:05] [Epoch=015/200] [Need: 01:05:41] [LR=0.0010][M=0.90] [Best : Accuracy=78.84, Error=21.16]
  Epoch: [015][000/391]   Time 0.259 (0.259)   Data 0.201 (0.201)   Loss 0.6030 (0.6030)   Prec@1 78.125 (78.125)   Prec@5 99.219 (99.219)   [2019-11-23 23:14:06]
  Epoch: [015][100/391]   Time 0.043 (0.051)   Data 0.000 (0.002)   Loss 0.3856 (0.4597)   Prec@1 87.500 (84.228)   Prec@5 98.438 (99.412)   [2019-11-23 23:14:10]
  Epoch: [015][200/391]   Time 0.042 (0.050)   Data 0.000 (0.001)   Loss 0.5091 (0.4761)   Prec@1 80.469 (83.633)   Prec@5 99.219 (99.265)   [2019-11-23 23:14:15]
  Epoch: [015][300/391]   Time 0.044 (0.050)   Data 0.000 (0.001)   Loss 0.4578 (0.4779)   Prec@1 86.719 (83.448)   Prec@5 99.219 (99.278)   [2019-11-23 23:14:20]
  **Train** Prec@1 83.516 Prec@5 99.278 Error@1 16.484
  **Test** Prec@1 81.220 Prec@5 99.030 Error@1 18.780
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:14:27] [Epoch=016/200] [Need: 01:05:21] [LR=0.0010][M=0.90] [Best : Accuracy=81.22, Error=18.78]
  Epoch: [016][000/391]   Time 0.249 (0.249)   Data 0.182 (0.182)   Loss 0.4897 (0.4897)   Prec@1 82.031 (82.031)   Prec@5 100.000 (100.000)   [2019-11-23 23:14:27]
  Epoch: [016][100/391]   Time 0.052 (0.051)   Data 0.000 (0.002)   Loss 0.3753 (0.4502)   Prec@1 84.375 (84.197)   Prec@5 100.000 (99.366)   [2019-11-23 23:14:32]
  Epoch: [016][200/391]   Time 0.046 (0.049)   Data 0.000 (0.001)   Loss 0.5020 (0.4537)   Prec@1 82.812 (84.270)   Prec@5 98.438 (99.296)   [2019-11-23 23:14:37]
  Epoch: [016][300/391]   Time 0.052 (0.050)   Data 0.000 (0.001)   Loss 0.5360 (0.4614)   Prec@1 80.469 (83.993)   Prec@5 100.000 (99.317)   [2019-11-23 23:14:42]
  **Train** Prec@1 84.158 Prec@5 99.324 Error@1 15.842
  **Test** Prec@1 81.570 Prec@5 99.120 Error@1 18.430
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:14:48] [Epoch=017/200] [Need: 01:05:02] [LR=0.0010][M=0.90] [Best : Accuracy=81.57, Error=18.43]
  Epoch: [017][000/391]   Time 0.276 (0.276)   Data 0.225 (0.225)   Loss 0.3811 (0.3811)   Prec@1 85.156 (85.156)   Prec@5 100.000 (100.000)   [2019-11-23 23:14:49]
  Epoch: [017][100/391]   Time 0.040 (0.048)   Data 0.000 (0.002)   Loss 0.4350 (0.4324)   Prec@1 84.375 (84.901)   Prec@5 100.000 (99.582)   [2019-11-23 23:14:53]
  Epoch: [017][200/391]   Time 0.042 (0.048)   Data 0.000 (0.001)   Loss 0.5946 (0.4363)   Prec@1 80.469 (84.725)   Prec@5 98.438 (99.468)   [2019-11-23 23:14:58]
  Epoch: [017][300/391]   Time 0.042 (0.049)   Data 0.000 (0.001)   Loss 0.6051 (0.4441)   Prec@1 79.688 (84.474)   Prec@5 99.219 (99.442)   [2019-11-23 23:15:03]
  **Train** Prec@1 84.490 Prec@5 99.398 Error@1 15.510
  **Test** Prec@1 80.100 Prec@5 99.010 Error@1 19.900

==>>[2019-11-23 23:15:10] [Epoch=018/200] [Need: 01:04:40] [LR=0.0010][M=0.90] [Best : Accuracy=81.57, Error=18.43]
  Epoch: [018][000/391]   Time 0.266 (0.266)   Data 0.188 (0.188)   Loss 0.3524 (0.3524)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2019-11-23 23:15:10]
  Epoch: [018][100/391]   Time 0.043 (0.048)   Data 0.000 (0.002)   Loss 0.4122 (0.4246)   Prec@1 83.594 (85.319)   Prec@5 98.438 (99.343)   [2019-11-23 23:15:14]
  Epoch: [018][200/391]   Time 0.042 (0.049)   Data 0.000 (0.001)   Loss 0.4158 (0.4307)   Prec@1 84.375 (84.993)   Prec@5 100.000 (99.398)   [2019-11-23 23:15:19]
  Epoch: [018][300/391]   Time 0.042 (0.049)   Data 0.000 (0.001)   Loss 0.5210 (0.4323)   Prec@1 84.375 (85.006)   Prec@5 99.219 (99.390)   [2019-11-23 23:15:24]
  **Train** Prec@1 84.846 Prec@5 99.344 Error@1 15.154
  **Test** Prec@1 80.470 Prec@5 99.140 Error@1 19.530

==>>[2019-11-23 23:15:31] [Epoch=019/200] [Need: 01:04:19] [LR=0.0010][M=0.90] [Best : Accuracy=81.57, Error=18.43]
  Epoch: [019][000/391]   Time 0.256 (0.256)   Data 0.199 (0.199)   Loss 0.5203 (0.5203)   Prec@1 80.469 (80.469)   Prec@5 99.219 (99.219)   [2019-11-23 23:15:31]
  Epoch: [019][100/391]   Time 0.081 (0.054)   Data 0.000 (0.002)   Loss 0.4232 (0.4324)   Prec@1 84.375 (85.195)   Prec@5 100.000 (99.343)   [2019-11-23 23:15:36]
  Epoch: [019][200/391]   Time 0.058 (0.051)   Data 0.000 (0.001)   Loss 0.4458 (0.4212)   Prec@1 85.938 (85.459)   Prec@5 99.219 (99.398)   [2019-11-23 23:15:41]
  Epoch: [019][300/391]   Time 0.044 (0.051)   Data 0.000 (0.001)   Loss 0.4870 (0.4222)   Prec@1 79.688 (85.434)   Prec@5 100.000 (99.385)   [2019-11-23 23:15:46]
  **Train** Prec@1 85.444 Prec@5 99.410 Error@1 14.556
  **Test** Prec@1 77.180 Prec@5 98.410 Error@1 22.820

==>>[2019-11-23 23:15:53] [Epoch=020/200] [Need: 01:04:01] [LR=0.0010][M=0.90] [Best : Accuracy=81.57, Error=18.43]
  Epoch: [020][000/391]   Time 0.257 (0.257)   Data 0.197 (0.197)   Loss 0.4317 (0.4317)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2019-11-23 23:15:53]
  Epoch: [020][100/391]   Time 0.052 (0.052)   Data 0.000 (0.002)   Loss 0.3965 (0.4174)   Prec@1 85.938 (85.404)   Prec@5 100.000 (99.451)   [2019-11-23 23:15:58]
  Epoch: [020][200/391]   Time 0.080 (0.052)   Data 0.000 (0.001)   Loss 0.4570 (0.4151)   Prec@1 85.156 (85.708)   Prec@5 98.438 (99.452)   [2019-11-23 23:16:03]
  Epoch: [020][300/391]   Time 0.041 (0.051)   Data 0.000 (0.001)   Loss 0.4787 (0.4122)   Prec@1 78.906 (85.678)   Prec@5 98.438 (99.437)   [2019-11-23 23:16:08]
  **Train** Prec@1 85.612 Prec@5 99.428 Error@1 14.388
  **Test** Prec@1 80.270 Prec@5 99.060 Error@1 19.730

==>>[2019-11-23 23:16:14] [Epoch=021/200] [Need: 01:03:42] [LR=0.0010][M=0.90] [Best : Accuracy=81.57, Error=18.43]
  Epoch: [021][000/391]   Time 0.276 (0.276)   Data 0.202 (0.202)   Loss 0.4952 (0.4952)   Prec@1 82.812 (82.812)   Prec@5 99.219 (99.219)   [2019-11-23 23:16:15]
  Epoch: [021][100/391]   Time 0.056 (0.053)   Data 0.000 (0.002)   Loss 0.4750 (0.4045)   Prec@1 85.156 (85.752)   Prec@5 99.219 (99.389)   [2019-11-23 23:16:20]
  Epoch: [021][200/391]   Time 0.043 (0.051)   Data 0.000 (0.001)   Loss 0.4444 (0.4042)   Prec@1 83.594 (85.790)   Prec@5 100.000 (99.405)   [2019-11-23 23:16:24]
  Epoch: [021][300/391]   Time 0.061 (0.052)   Data 0.000 (0.001)   Loss 0.5472 (0.4047)   Prec@1 83.594 (85.800)   Prec@5 100.000 (99.411)   [2019-11-23 23:16:30]
  **Train** Prec@1 85.632 Prec@5 99.444 Error@1 14.368
  **Test** Prec@1 80.230 Prec@5 99.010 Error@1 19.770

==>>[2019-11-23 23:16:36] [Epoch=022/200] [Need: 01:03:27] [LR=0.0010][M=0.90] [Best : Accuracy=81.57, Error=18.43]
  Epoch: [022][000/391]   Time 0.259 (0.259)   Data 0.207 (0.207)   Loss 0.4579 (0.4579)   Prec@1 85.156 (85.156)   Prec@5 100.000 (100.000)   [2019-11-23 23:16:37]
  Epoch: [022][100/391]   Time 0.044 (0.051)   Data 0.000 (0.002)   Loss 0.4312 (0.3986)   Prec@1 83.594 (86.061)   Prec@5 100.000 (99.551)   [2019-11-23 23:16:41]
  Epoch: [022][200/391]   Time 0.042 (0.050)   Data 0.000 (0.001)   Loss 0.5395 (0.3933)   Prec@1 78.906 (86.229)   Prec@5 99.219 (99.572)   [2019-11-23 23:16:46]
  Epoch: [022][300/391]   Time 0.045 (0.049)   Data 0.000 (0.001)   Loss 0.3711 (0.3953)   Prec@1 89.062 (86.213)   Prec@5 99.219 (99.541)   [2019-11-23 23:16:51]
  **Train** Prec@1 86.212 Prec@5 99.526 Error@1 13.788
  **Test** Prec@1 80.760 Prec@5 99.100 Error@1 19.240

==>>[2019-11-23 23:16:57] [Epoch=023/200] [Need: 01:03:03] [LR=0.0010][M=0.90] [Best : Accuracy=81.57, Error=18.43]
  Epoch: [023][000/391]   Time 0.270 (0.270)   Data 0.205 (0.205)   Loss 0.3240 (0.3240)   Prec@1 87.500 (87.500)   Prec@5 99.219 (99.219)   [2019-11-23 23:16:58]
  Epoch: [023][100/391]   Time 0.045 (0.052)   Data 0.002 (0.002)   Loss 0.3749 (0.3807)   Prec@1 85.938 (86.757)   Prec@5 100.000 (99.528)   [2019-11-23 23:17:03]
  Epoch: [023][200/391]   Time 0.073 (0.051)   Data 0.000 (0.001)   Loss 0.4803 (0.3914)   Prec@1 86.719 (86.454)   Prec@5 99.219 (99.483)   [2019-11-23 23:17:08]
  Epoch: [023][300/391]   Time 0.045 (0.052)   Data 0.000 (0.001)   Loss 0.3885 (0.3955)   Prec@1 89.062 (86.270)   Prec@5 99.219 (99.455)   [2019-11-23 23:17:13]
  **Train** Prec@1 86.190 Prec@5 99.464 Error@1 13.810
  **Test** Prec@1 83.950 Prec@5 99.310 Error@1 16.050
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:17:20] [Epoch=024/200] [Need: 01:02:49] [LR=0.0010][M=0.90] [Best : Accuracy=83.95, Error=16.05]
  Epoch: [024][000/391]   Time 0.277 (0.277)   Data 0.221 (0.221)   Loss 0.3548 (0.3548)   Prec@1 87.500 (87.500)   Prec@5 99.219 (99.219)   [2019-11-23 23:17:20]
  Epoch: [024][100/391]   Time 0.042 (0.052)   Data 0.000 (0.002)   Loss 0.4591 (0.3885)   Prec@1 86.719 (86.564)   Prec@5 100.000 (99.459)   [2019-11-23 23:17:25]
  Epoch: [024][200/391]   Time 0.051 (0.051)   Data 0.000 (0.001)   Loss 0.4493 (0.3873)   Prec@1 84.375 (86.602)   Prec@5 99.219 (99.487)   [2019-11-23 23:17:30]
  Epoch: [024][300/391]   Time 0.046 (0.050)   Data 0.000 (0.001)   Loss 0.5268 (0.3855)   Prec@1 82.031 (86.656)   Prec@5 98.438 (99.533)   [2019-11-23 23:17:35]
  **Train** Prec@1 86.538 Prec@5 99.534 Error@1 13.462
  **Test** Prec@1 81.210 Prec@5 98.970 Error@1 18.790

==>>[2019-11-23 23:17:41] [Epoch=025/200] [Need: 01:02:28] [LR=0.0010][M=0.90] [Best : Accuracy=83.95, Error=16.05]
  Epoch: [025][000/391]   Time 0.268 (0.268)   Data 0.213 (0.213)   Loss 0.3427 (0.3427)   Prec@1 85.156 (85.156)   Prec@5 100.000 (100.000)   [2019-11-23 23:17:41]
  Epoch: [025][100/391]   Time 0.041 (0.053)   Data 0.000 (0.002)   Loss 0.4081 (0.3852)   Prec@1 88.281 (86.696)   Prec@5 98.438 (99.459)   [2019-11-23 23:17:47]
  Epoch: [025][200/391]   Time 0.043 (0.050)   Data 0.000 (0.001)   Loss 0.3275 (0.3897)   Prec@1 88.281 (86.478)   Prec@5 100.000 (99.460)   [2019-11-23 23:17:51]
  Epoch: [025][300/391]   Time 0.041 (0.050)   Data 0.000 (0.001)   Loss 0.3315 (0.3896)   Prec@1 85.938 (86.493)   Prec@5 99.219 (99.489)   [2019-11-23 23:17:56]
  **Train** Prec@1 86.604 Prec@5 99.478 Error@1 13.396
  **Test** Prec@1 81.290 Prec@5 98.960 Error@1 18.710

==>>[2019-11-23 23:18:02] [Epoch=026/200] [Need: 01:02:05] [LR=0.0010][M=0.90] [Best : Accuracy=83.95, Error=16.05]
  Epoch: [026][000/391]   Time 0.261 (0.261)   Data 0.190 (0.190)   Loss 0.2748 (0.2748)   Prec@1 88.281 (88.281)   Prec@5 100.000 (100.000)   [2019-11-23 23:18:03]
  Epoch: [026][100/391]   Time 0.040 (0.053)   Data 0.000 (0.002)   Loss 0.3968 (0.3616)   Prec@1 86.719 (87.268)   Prec@5 100.000 (99.629)   [2019-11-23 23:18:08]
  Epoch: [026][200/391]   Time 0.046 (0.051)   Data 0.000 (0.001)   Loss 0.3981 (0.3784)   Prec@1 85.156 (86.668)   Prec@5 100.000 (99.549)   [2019-11-23 23:18:13]
  Epoch: [026][300/391]   Time 0.045 (0.050)   Data 0.000 (0.001)   Loss 0.3517 (0.3824)   Prec@1 86.719 (86.573)   Prec@5 100.000 (99.525)   [2019-11-23 23:18:18]
  **Train** Prec@1 86.572 Prec@5 99.512 Error@1 13.428
  **Test** Prec@1 84.100 Prec@5 99.170 Error@1 15.900
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:18:24] [Epoch=027/200] [Need: 01:01:42] [LR=0.0010][M=0.90] [Best : Accuracy=84.10, Error=15.90]
  Epoch: [027][000/391]   Time 0.250 (0.250)   Data 0.191 (0.191)   Loss 0.3816 (0.3816)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2019-11-23 23:18:24]
  Epoch: [027][100/391]   Time 0.080 (0.052)   Data 0.000 (0.002)   Loss 0.3818 (0.3565)   Prec@1 87.500 (87.276)   Prec@5 100.000 (99.613)   [2019-11-23 23:18:29]
  Epoch: [027][200/391]   Time 0.045 (0.048)   Data 0.000 (0.001)   Loss 0.4032 (0.3620)   Prec@1 85.938 (87.088)   Prec@5 99.219 (99.604)   [2019-11-23 23:18:33]
  Epoch: [027][300/391]   Time 0.079 (0.048)   Data 0.000 (0.001)   Loss 0.2253 (0.3665)   Prec@1 94.531 (87.074)   Prec@5 100.000 (99.572)   [2019-11-23 23:18:38]
  **Train** Prec@1 87.150 Prec@5 99.572 Error@1 12.850
  **Test** Prec@1 82.830 Prec@5 99.320 Error@1 17.170

==>>[2019-11-23 23:18:44] [Epoch=028/200] [Need: 01:01:17] [LR=0.0010][M=0.90] [Best : Accuracy=84.10, Error=15.90]
  Epoch: [028][000/391]   Time 0.277 (0.277)   Data 0.222 (0.222)   Loss 0.4344 (0.4344)   Prec@1 85.156 (85.156)   Prec@5 100.000 (100.000)   [2019-11-23 23:18:45]
  Epoch: [028][100/391]   Time 0.048 (0.052)   Data 0.000 (0.002)   Loss 0.2973 (0.3511)   Prec@1 90.625 (87.894)   Prec@5 100.000 (99.675)   [2019-11-23 23:18:50]
  Epoch: [028][200/391]   Time 0.042 (0.050)   Data 0.000 (0.001)   Loss 0.5976 (0.3584)   Prec@1 80.469 (87.570)   Prec@5 99.219 (99.619)   [2019-11-23 23:18:55]
  Epoch: [028][300/391]   Time 0.052 (0.050)   Data 0.000 (0.001)   Loss 0.4024 (0.3637)   Prec@1 85.156 (87.370)   Prec@5 100.000 (99.582)   [2019-11-23 23:18:59]
  **Train** Prec@1 87.348 Prec@5 99.582 Error@1 12.652
  **Test** Prec@1 83.160 Prec@5 98.950 Error@1 16.840

==>>[2019-11-23 23:19:06] [Epoch=029/200] [Need: 01:00:55] [LR=0.0010][M=0.90] [Best : Accuracy=84.10, Error=15.90]
  Epoch: [029][000/391]   Time 0.266 (0.266)   Data 0.209 (0.209)   Loss 0.4275 (0.4275)   Prec@1 85.938 (85.938)   Prec@5 100.000 (100.000)   [2019-11-23 23:19:06]
  Epoch: [029][100/391]   Time 0.051 (0.051)   Data 0.000 (0.002)   Loss 0.4363 (0.3595)   Prec@1 82.031 (87.299)   Prec@5 99.219 (99.513)   [2019-11-23 23:19:11]
  Epoch: [029][200/391]   Time 0.052 (0.050)   Data 0.000 (0.001)   Loss 0.4767 (0.3613)   Prec@1 83.594 (87.368)   Prec@5 100.000 (99.557)   [2019-11-23 23:19:16]
  Epoch: [029][300/391]   Time 0.044 (0.050)   Data 0.000 (0.001)   Loss 0.4264 (0.3624)   Prec@1 82.812 (87.303)   Prec@5 98.438 (99.569)   [2019-11-23 23:19:21]
  **Train** Prec@1 87.478 Prec@5 99.576 Error@1 12.522
  **Test** Prec@1 82.400 Prec@5 99.070 Error@1 17.600

==>>[2019-11-23 23:19:27] [Epoch=030/200] [Need: 01:00:35] [LR=0.0010][M=0.90] [Best : Accuracy=84.10, Error=15.90]
  Epoch: [030][000/391]   Time 0.291 (0.291)   Data 0.233 (0.233)   Loss 0.3149 (0.3149)   Prec@1 88.281 (88.281)   Prec@5 100.000 (100.000)   [2019-11-23 23:19:28]
  Epoch: [030][100/391]   Time 0.083 (0.052)   Data 0.000 (0.002)   Loss 0.3878 (0.3503)   Prec@1 87.500 (87.972)   Prec@5 97.656 (99.613)   [2019-11-23 23:19:33]
  Epoch: [030][200/391]   Time 0.056 (0.051)   Data 0.000 (0.001)   Loss 0.3489 (0.3539)   Prec@1 88.281 (87.652)   Prec@5 100.000 (99.627)   [2019-11-23 23:19:38]
  Epoch: [030][300/391]   Time 0.042 (0.051)   Data 0.000 (0.001)   Loss 0.2974 (0.3516)   Prec@1 89.844 (87.702)   Prec@5 100.000 (99.626)   [2019-11-23 23:19:43]
  **Train** Prec@1 87.598 Prec@5 99.620 Error@1 12.402
  **Test** Prec@1 84.240 Prec@5 99.290 Error@1 15.760
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:19:49] [Epoch=031/200] [Need: 01:00:15] [LR=0.0010][M=0.90] [Best : Accuracy=84.24, Error=15.76]
  Epoch: [031][000/391]   Time 0.270 (0.270)   Data 0.217 (0.217)   Loss 0.3560 (0.3560)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2019-11-23 23:19:49]
  Epoch: [031][100/391]   Time 0.043 (0.054)   Data 0.000 (0.002)   Loss 0.1963 (0.3298)   Prec@1 93.750 (88.490)   Prec@5 100.000 (99.644)   [2019-11-23 23:19:54]
  Epoch: [031][200/391]   Time 0.044 (0.051)   Data 0.000 (0.001)   Loss 0.2683 (0.3439)   Prec@1 92.969 (88.005)   Prec@5 100.000 (99.635)   [2019-11-23 23:19:59]
  Epoch: [031][300/391]   Time 0.042 (0.050)   Data 0.000 (0.001)   Loss 0.3990 (0.3475)   Prec@1 85.156 (87.770)   Prec@5 100.000 (99.642)   [2019-11-23 23:20:04]
  **Train** Prec@1 87.644 Prec@5 99.614 Error@1 12.356
  **Test** Prec@1 82.890 Prec@5 99.000 Error@1 17.110

==>>[2019-11-23 23:20:10] [Epoch=032/200] [Need: 00:59:53] [LR=0.0010][M=0.90] [Best : Accuracy=84.24, Error=15.76]
  Epoch: [032][000/391]   Time 0.272 (0.272)   Data 0.220 (0.220)   Loss 0.3488 (0.3488)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2019-11-23 23:20:11]
  Epoch: [032][100/391]   Time 0.043 (0.056)   Data 0.000 (0.002)   Loss 0.4627 (0.3293)   Prec@1 84.375 (88.683)   Prec@5 100.000 (99.675)   [2019-11-23 23:20:16]
  Epoch: [032][200/391]   Time 0.045 (0.054)   Data 0.000 (0.001)   Loss 0.2738 (0.3299)   Prec@1 88.281 (88.577)   Prec@5 100.000 (99.639)   [2019-11-23 23:20:21]
  Epoch: [032][300/391]   Time 0.047 (0.052)   Data 0.000 (0.001)   Loss 0.3114 (0.3368)   Prec@1 90.625 (88.351)   Prec@5 99.219 (99.603)   [2019-11-23 23:20:26]
  **Train** Prec@1 88.056 Prec@5 99.612 Error@1 11.944
  **Test** Prec@1 83.180 Prec@5 99.270 Error@1 16.820

==>>[2019-11-23 23:20:33] [Epoch=033/200] [Need: 00:59:37] [LR=0.0010][M=0.90] [Best : Accuracy=84.24, Error=15.76]
  Epoch: [033][000/391]   Time 0.261 (0.261)   Data 0.192 (0.192)   Loss 0.4370 (0.4370)   Prec@1 85.156 (85.156)   Prec@5 99.219 (99.219)   [2019-11-23 23:20:33]
  Epoch: [033][100/391]   Time 0.045 (0.050)   Data 0.000 (0.002)   Loss 0.2815 (0.3374)   Prec@1 90.625 (88.297)   Prec@5 99.219 (99.606)   [2019-11-23 23:20:38]
  Epoch: [033][200/391]   Time 0.080 (0.049)   Data 0.000 (0.001)   Loss 0.4204 (0.3414)   Prec@1 85.156 (88.083)   Prec@5 100.000 (99.607)   [2019-11-23 23:20:42]
  Epoch: [033][300/391]   Time 0.042 (0.050)   Data 0.000 (0.001)   Loss 0.3383 (0.3405)   Prec@1 88.281 (88.144)   Prec@5 99.219 (99.621)   [2019-11-23 23:20:48]
  **Train** Prec@1 87.824 Prec@5 99.602 Error@1 12.176
  **Test** Prec@1 82.580 Prec@5 99.320 Error@1 17.420

==>>[2019-11-23 23:20:54] [Epoch=034/200] [Need: 00:59:15] [LR=0.0010][M=0.90] [Best : Accuracy=84.24, Error=15.76]
  Epoch: [034][000/391]   Time 0.253 (0.253)   Data 0.186 (0.186)   Loss 0.3368 (0.3368)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-11-23 23:20:54]
  Epoch: [034][100/391]   Time 0.043 (0.050)   Data 0.000 (0.002)   Loss 0.4593 (0.3346)   Prec@1 84.375 (87.964)   Prec@5 99.219 (99.722)   [2019-11-23 23:20:59]
  Epoch: [034][200/391]   Time 0.042 (0.048)   Data 0.000 (0.001)   Loss 0.4431 (0.3323)   Prec@1 87.500 (88.204)   Prec@5 100.000 (99.681)   [2019-11-23 23:21:04]
  Epoch: [034][300/391]   Time 0.042 (0.048)   Data 0.000 (0.001)   Loss 0.4513 (0.3372)   Prec@1 82.812 (88.087)   Prec@5 99.219 (99.686)   [2019-11-23 23:21:08]
  **Train** Prec@1 88.088 Prec@5 99.660 Error@1 11.912
  **Test** Prec@1 81.310 Prec@5 99.000 Error@1 18.690

==>>[2019-11-23 23:21:15] [Epoch=035/200] [Need: 00:58:51] [LR=0.0010][M=0.90] [Best : Accuracy=84.24, Error=15.76]
  Epoch: [035][000/391]   Time 0.259 (0.259)   Data 0.203 (0.203)   Loss 0.2708 (0.2708)   Prec@1 90.625 (90.625)   Prec@5 99.219 (99.219)   [2019-11-23 23:21:15]
  Epoch: [035][100/391]   Time 0.043 (0.050)   Data 0.000 (0.002)   Loss 0.2509 (0.3319)   Prec@1 92.188 (88.003)   Prec@5 100.000 (99.613)   [2019-11-23 23:21:20]
  Epoch: [035][200/391]   Time 0.043 (0.051)   Data 0.000 (0.001)   Loss 0.2932 (0.3351)   Prec@1 92.188 (88.110)   Prec@5 99.219 (99.650)   [2019-11-23 23:21:25]
  Epoch: [035][300/391]   Time 0.044 (0.050)   Data 0.000 (0.001)   Loss 0.3574 (0.3326)   Prec@1 88.281 (88.240)   Prec@5 98.438 (99.634)   [2019-11-23 23:21:30]
  **Train** Prec@1 88.122 Prec@5 99.622 Error@1 11.878
  **Test** Prec@1 82.770 Prec@5 99.250 Error@1 17.230

==>>[2019-11-23 23:21:36] [Epoch=036/200] [Need: 00:58:29] [LR=0.0010][M=0.90] [Best : Accuracy=84.24, Error=15.76]
  Epoch: [036][000/391]   Time 0.280 (0.280)   Data 0.212 (0.212)   Loss 0.2880 (0.2880)   Prec@1 88.281 (88.281)   Prec@5 100.000 (100.000)   [2019-11-23 23:21:36]
  Epoch: [036][100/391]   Time 0.043 (0.053)   Data 0.000 (0.002)   Loss 0.4016 (0.3258)   Prec@1 82.812 (88.668)   Prec@5 99.219 (99.613)   [2019-11-23 23:21:41]
  Epoch: [036][200/391]   Time 0.051 (0.051)   Data 0.000 (0.001)   Loss 0.3057 (0.3292)   Prec@1 86.719 (88.538)   Prec@5 100.000 (99.650)   [2019-11-23 23:21:46]
  Epoch: [036][300/391]   Time 0.043 (0.049)   Data 0.000 (0.001)   Loss 0.2254 (0.3341)   Prec@1 93.750 (88.382)   Prec@5 100.000 (99.613)   [2019-11-23 23:21:51]
  **Train** Prec@1 88.410 Prec@5 99.598 Error@1 11.590
  **Test** Prec@1 84.290 Prec@5 99.120 Error@1 15.710
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:21:58] [Epoch=037/200] [Need: 00:58:08] [LR=0.0010][M=0.90] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [037][000/391]   Time 0.271 (0.271)   Data 0.210 (0.210)   Loss 0.3265 (0.3265)   Prec@1 85.938 (85.938)   Prec@5 100.000 (100.000)   [2019-11-23 23:21:58]
  Epoch: [037][100/391]   Time 0.042 (0.055)   Data 0.000 (0.002)   Loss 0.2254 (0.3265)   Prec@1 94.531 (88.699)   Prec@5 99.219 (99.691)   [2019-11-23 23:22:03]
  Epoch: [037][200/391]   Time 0.045 (0.052)   Data 0.000 (0.001)   Loss 0.3290 (0.3305)   Prec@1 87.500 (88.464)   Prec@5 99.219 (99.662)   [2019-11-23 23:22:08]
  Epoch: [037][300/391]   Time 0.040 (0.050)   Data 0.000 (0.001)   Loss 0.3705 (0.3350)   Prec@1 89.844 (88.328)   Prec@5 99.219 (99.634)   [2019-11-23 23:22:13]
  **Train** Prec@1 88.318 Prec@5 99.654 Error@1 11.682
  **Test** Prec@1 82.840 Prec@5 98.940 Error@1 17.160

==>>[2019-11-23 23:22:19] [Epoch=038/200] [Need: 00:57:46] [LR=0.0010][M=0.90] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [038][000/391]   Time 0.277 (0.277)   Data 0.221 (0.221)   Loss 0.3226 (0.3226)   Prec@1 89.844 (89.844)   Prec@5 99.219 (99.219)   [2019-11-23 23:22:19]
  Epoch: [038][100/391]   Time 0.047 (0.053)   Data 0.000 (0.002)   Loss 0.3088 (0.3061)   Prec@1 90.625 (89.225)   Prec@5 100.000 (99.644)   [2019-11-23 23:22:24]
  Epoch: [038][200/391]   Time 0.051 (0.050)   Data 0.000 (0.001)   Loss 0.3335 (0.3183)   Prec@1 88.281 (88.923)   Prec@5 99.219 (99.631)   [2019-11-23 23:22:29]
  Epoch: [038][300/391]   Time 0.039 (0.049)   Data 0.000 (0.001)   Loss 0.4026 (0.3271)   Prec@1 86.719 (88.704)   Prec@5 99.219 (99.644)   [2019-11-23 23:22:34]
  **Train** Prec@1 88.564 Prec@5 99.644 Error@1 11.436
  **Test** Prec@1 82.680 Prec@5 99.250 Error@1 17.320

==>>[2019-11-23 23:22:40] [Epoch=039/200] [Need: 00:57:23] [LR=0.0010][M=0.90] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [039][000/391]   Time 0.266 (0.266)   Data 0.193 (0.193)   Loss 0.3321 (0.3321)   Prec@1 88.281 (88.281)   Prec@5 100.000 (100.000)   [2019-11-23 23:22:40]
  Epoch: [039][100/391]   Time 0.044 (0.052)   Data 0.000 (0.002)   Loss 0.3903 (0.3287)   Prec@1 86.719 (88.653)   Prec@5 99.219 (99.629)   [2019-11-23 23:22:45]
  Epoch: [039][200/391]   Time 0.042 (0.051)   Data 0.000 (0.001)   Loss 0.2718 (0.3372)   Prec@1 89.844 (88.270)   Prec@5 99.219 (99.635)   [2019-11-23 23:22:50]
  Epoch: [039][300/391]   Time 0.050 (0.051)   Data 0.000 (0.001)   Loss 0.3818 (0.3350)   Prec@1 84.375 (88.237)   Prec@5 100.000 (99.639)   [2019-11-23 23:22:55]
  **Train** Prec@1 88.234 Prec@5 99.648 Error@1 11.766
  **Test** Prec@1 83.820 Prec@5 99.170 Error@1 16.180

==>>[2019-11-23 23:23:02] [Epoch=040/200] [Need: 00:57:03] [LR=0.0010][M=0.90] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [040][000/391]   Time 0.262 (0.262)   Data 0.204 (0.204)   Loss 0.2254 (0.2254)   Prec@1 92.969 (92.969)   Prec@5 99.219 (99.219)   [2019-11-23 23:23:02]
  Epoch: [040][100/391]   Time 0.042 (0.051)   Data 0.000 (0.002)   Loss 0.3251 (0.3119)   Prec@1 88.281 (89.086)   Prec@5 100.000 (99.675)   [2019-11-23 23:23:07]
  Epoch: [040][200/391]   Time 0.044 (0.051)   Data 0.000 (0.001)   Loss 0.3241 (0.3235)   Prec@1 87.500 (88.818)   Prec@5 99.219 (99.623)   [2019-11-23 23:23:12]
  Epoch: [040][300/391]   Time 0.052 (0.050)   Data 0.000 (0.001)   Loss 0.3399 (0.3216)   Prec@1 89.844 (88.837)   Prec@5 100.000 (99.624)   [2019-11-23 23:23:17]
  **Train** Prec@1 88.936 Prec@5 99.636 Error@1 11.064
  **Test** Prec@1 82.930 Prec@5 99.280 Error@1 17.070

==>>[2019-11-23 23:23:23] [Epoch=041/200] [Need: 00:56:43] [LR=0.0010][M=0.90] [Best : Accuracy=84.29, Error=15.71]
  Epoch: [041][000/391]   Time 0.254 (0.254)   Data 0.191 (0.191)   Loss 0.2898 (0.2898)   Prec@1 89.844 (89.844)   Prec@5 99.219 (99.219)   [2019-11-23 23:23:24]
  Epoch: [041][100/391]   Time 0.048 (0.055)   Data 0.000 (0.002)   Loss 0.3588 (0.3236)   Prec@1 88.281 (88.745)   Prec@5 100.000 (99.714)   [2019-11-23 23:23:29]
  Epoch: [041][200/391]   Time 0.048 (0.052)   Data 0.000 (0.001)   Loss 0.2754 (0.3207)   Prec@1 92.188 (88.845)   Prec@5 99.219 (99.705)   [2019-11-23 23:23:34]
  Epoch: [041][300/391]   Time 0.043 (0.051)   Data 0.000 (0.001)   Loss 0.2783 (0.3191)   Prec@1 87.500 (88.842)   Prec@5 99.219 (99.714)   [2019-11-23 23:23:39]
  **Train** Prec@1 88.900 Prec@5 99.692 Error@1 11.100
  **Test** Prec@1 84.460 Prec@5 99.070 Error@1 15.540
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:23:45] [Epoch=042/200] [Need: 00:56:23] [LR=0.0010][M=0.90] [Best : Accuracy=84.46, Error=15.54]
  Epoch: [042][000/391]   Time 0.261 (0.261)   Data 0.198 (0.198)   Loss 0.3053 (0.3053)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2019-11-23 23:23:45]
  Epoch: [042][100/391]   Time 0.042 (0.051)   Data 0.000 (0.002)   Loss 0.3058 (0.2988)   Prec@1 89.062 (89.759)   Prec@5 99.219 (99.613)   [2019-11-23 23:23:50]
  Epoch: [042][200/391]   Time 0.044 (0.050)   Data 0.000 (0.001)   Loss 0.2880 (0.3062)   Prec@1 89.844 (89.482)   Prec@5 100.000 (99.674)   [2019-11-23 23:23:55]
  Epoch: [042][300/391]   Time 0.046 (0.050)   Data 0.000 (0.001)   Loss 0.3939 (0.3078)   Prec@1 87.500 (89.351)   Prec@5 100.000 (99.678)   [2019-11-23 23:24:00]
  **Train** Prec@1 89.276 Prec@5 99.660 Error@1 10.724
  **Test** Prec@1 83.160 Prec@5 99.190 Error@1 16.840

==>>[2019-11-23 23:24:07] [Epoch=043/200] [Need: 00:56:01] [LR=0.0010][M=0.90] [Best : Accuracy=84.46, Error=15.54]
  Epoch: [043][000/391]   Time 0.268 (0.268)   Data 0.191 (0.191)   Loss 0.2925 (0.2925)   Prec@1 87.500 (87.500)   Prec@5 100.000 (100.000)   [2019-11-23 23:24:07]
  Epoch: [043][100/391]   Time 0.044 (0.053)   Data 0.000 (0.002)   Loss 0.1552 (0.2887)   Prec@1 95.312 (89.851)   Prec@5 100.000 (99.791)   [2019-11-23 23:24:12]
  Epoch: [043][200/391]   Time 0.042 (0.050)   Data 0.000 (0.001)   Loss 0.2303 (0.3020)   Prec@1 91.406 (89.389)   Prec@5 98.438 (99.751)   [2019-11-23 23:24:17]
  Epoch: [043][300/391]   Time 0.054 (0.051)   Data 0.000 (0.001)   Loss 0.2307 (0.3082)   Prec@1 92.188 (89.314)   Prec@5 100.000 (99.733)   [2019-11-23 23:24:22]
  **Train** Prec@1 89.144 Prec@5 99.710 Error@1 10.856
  **Test** Prec@1 81.490 Prec@5 99.020 Error@1 18.510

==>>[2019-11-23 23:24:28] [Epoch=044/200] [Need: 00:55:41] [LR=0.0010][M=0.90] [Best : Accuracy=84.46, Error=15.54]
  Epoch: [044][000/391]   Time 0.271 (0.271)   Data 0.211 (0.211)   Loss 0.2320 (0.2320)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-11-23 23:24:29]
  Epoch: [044][100/391]   Time 0.043 (0.051)   Data 0.000 (0.002)   Loss 0.3896 (0.3028)   Prec@1 86.719 (89.387)   Prec@5 100.000 (99.691)   [2019-11-23 23:24:33]
  Epoch: [044][200/391]   Time 0.052 (0.050)   Data 0.000 (0.001)   Loss 0.1902 (0.3156)   Prec@1 91.406 (88.888)   Prec@5 100.000 (99.674)   [2019-11-23 23:24:38]
  Epoch: [044][300/391]   Time 0.058 (0.049)   Data 0.000 (0.001)   Loss 0.3890 (0.3205)   Prec@1 88.281 (88.772)   Prec@5 99.219 (99.673)   [2019-11-23 23:24:43]
  **Train** Prec@1 88.788 Prec@5 99.674 Error@1 11.212
  **Test** Prec@1 82.730 Prec@5 98.900 Error@1 17.270

==>>[2019-11-23 23:24:50] [Epoch=045/200] [Need: 00:55:20] [LR=0.0010][M=0.90] [Best : Accuracy=84.46, Error=15.54]
  Epoch: [045][000/391]   Time 0.267 (0.267)   Data 0.191 (0.191)   Loss 0.3493 (0.3493)   Prec@1 89.844 (89.844)   Prec@5 99.219 (99.219)   [2019-11-23 23:24:50]
  Epoch: [045][100/391]   Time 0.052 (0.052)   Data 0.000 (0.002)   Loss 0.4080 (0.3139)   Prec@1 85.938 (89.086)   Prec@5 98.438 (99.683)   [2019-11-23 23:24:55]
  Epoch: [045][200/391]   Time 0.042 (0.050)   Data 0.000 (0.001)   Loss 0.3712 (0.3173)   Prec@1 83.594 (88.969)   Prec@5 99.219 (99.654)   [2019-11-23 23:25:00]
  Epoch: [045][300/391]   Time 0.040 (0.048)   Data 0.000 (0.001)   Loss 0.5163 (0.3209)   Prec@1 84.375 (88.819)   Prec@5 99.219 (99.686)   [2019-11-23 23:25:04]
  **Train** Prec@1 88.830 Prec@5 99.684 Error@1 11.170
  **Test** Prec@1 82.130 Prec@5 99.270 Error@1 17.870

==>>[2019-11-23 23:25:11] [Epoch=046/200] [Need: 00:54:58] [LR=0.0010][M=0.90] [Best : Accuracy=84.46, Error=15.54]
  Epoch: [046][000/391]   Time 0.270 (0.270)   Data 0.205 (0.205)   Loss 0.3896 (0.3896)   Prec@1 85.938 (85.938)   Prec@5 100.000 (100.000)   [2019-11-23 23:25:11]
  Epoch: [046][100/391]   Time 0.054 (0.049)   Data 0.000 (0.002)   Loss 0.2119 (0.3053)   Prec@1 92.969 (89.465)   Prec@5 100.000 (99.667)   [2019-11-23 23:25:16]
  Epoch: [046][200/391]   Time 0.043 (0.048)   Data 0.000 (0.001)   Loss 0.2905 (0.3097)   Prec@1 89.062 (89.284)   Prec@5 99.219 (99.650)   [2019-11-23 23:25:21]
  Epoch: [046][300/391]   Time 0.043 (0.048)   Data 0.000 (0.001)   Loss 0.2962 (0.3026)   Prec@1 90.625 (89.491)   Prec@5 100.000 (99.689)   [2019-11-23 23:25:26]
  **Train** Prec@1 89.452 Prec@5 99.704 Error@1 10.548
  **Test** Prec@1 84.810 Prec@5 99.240 Error@1 15.190
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:25:32] [Epoch=047/200] [Need: 00:54:35] [LR=0.0010][M=0.90] [Best : Accuracy=84.81, Error=15.19]
  Epoch: [047][000/391]   Time 0.272 (0.272)   Data 0.210 (0.210)   Loss 0.1438 (0.1438)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2019-11-23 23:25:32]
  Epoch: [047][100/391]   Time 0.043 (0.054)   Data 0.000 (0.002)   Loss 0.1964 (0.2840)   Prec@1 94.531 (90.192)   Prec@5 100.000 (99.760)   [2019-11-23 23:25:38]
  Epoch: [047][200/391]   Time 0.075 (0.052)   Data 0.000 (0.001)   Loss 0.3562 (0.2949)   Prec@1 89.844 (89.762)   Prec@5 100.000 (99.712)   [2019-11-23 23:25:43]
  Epoch: [047][300/391]   Time 0.051 (0.050)   Data 0.000 (0.001)   Loss 0.3149 (0.3021)   Prec@1 90.625 (89.517)   Prec@5 99.219 (99.702)   [2019-11-23 23:25:47]
  **Train** Prec@1 89.404 Prec@5 99.710 Error@1 10.596
  **Test** Prec@1 83.960 Prec@5 99.350 Error@1 16.040

==>>[2019-11-23 23:25:54] [Epoch=048/200] [Need: 00:54:15] [LR=0.0010][M=0.90] [Best : Accuracy=84.81, Error=15.19]
  Epoch: [048][000/391]   Time 0.267 (0.267)   Data 0.209 (0.209)   Loss 0.2327 (0.2327)   Prec@1 92.188 (92.188)   Prec@5 99.219 (99.219)   [2019-11-23 23:25:54]
  Epoch: [048][100/391]   Time 0.043 (0.053)   Data 0.000 (0.002)   Loss 0.2496 (0.2918)   Prec@1 91.406 (89.751)   Prec@5 99.219 (99.722)   [2019-11-23 23:25:59]
  Epoch: [048][200/391]   Time 0.047 (0.050)   Data 0.000 (0.001)   Loss 0.2556 (0.2952)   Prec@1 89.844 (89.684)   Prec@5 100.000 (99.708)   [2019-11-23 23:26:04]
  Epoch: [048][300/391]   Time 0.043 (0.049)   Data 0.000 (0.001)   Loss 0.2917 (0.2939)   Prec@1 90.625 (89.768)   Prec@5 99.219 (99.702)   [2019-11-23 23:26:09]
  **Train** Prec@1 89.730 Prec@5 99.684 Error@1 10.270
  **Test** Prec@1 84.420 Prec@5 99.120 Error@1 15.580

==>>[2019-11-23 23:26:15] [Epoch=049/200] [Need: 00:53:53] [LR=0.0010][M=0.90] [Best : Accuracy=84.81, Error=15.19]
  Epoch: [049][000/391]   Time 0.266 (0.266)   Data 0.209 (0.209)   Loss 0.4030 (0.4030)   Prec@1 87.500 (87.500)   Prec@5 99.219 (99.219)   [2019-11-23 23:26:15]
  Epoch: [049][100/391]   Time 0.044 (0.051)   Data 0.000 (0.002)   Loss 0.2519 (0.3015)   Prec@1 90.625 (89.542)   Prec@5 100.000 (99.760)   [2019-11-23 23:26:20]
  Epoch: [049][200/391]   Time 0.043 (0.052)   Data 0.000 (0.001)   Loss 0.3445 (0.2933)   Prec@1 91.406 (89.778)   Prec@5 99.219 (99.763)   [2019-11-23 23:26:26]
  Epoch: [049][300/391]   Time 0.044 (0.051)   Data 0.000 (0.001)   Loss 0.1896 (0.2920)   Prec@1 93.750 (89.839)   Prec@5 100.000 (99.759)   [2019-11-23 23:26:30]
  **Train** Prec@1 89.640 Prec@5 99.762 Error@1 10.360
  **Test** Prec@1 85.280 Prec@5 99.420 Error@1 14.720
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:26:37] [Epoch=050/200] [Need: 00:53:34] [LR=0.0010][M=0.90] [Best : Accuracy=85.28, Error=14.72]
  Epoch: [050][000/391]   Time 0.267 (0.267)   Data 0.206 (0.206)   Loss 0.2595 (0.2595)   Prec@1 89.844 (89.844)   Prec@5 100.000 (100.000)   [2019-11-23 23:26:37]
  Epoch: [050][100/391]   Time 0.081 (0.051)   Data 0.000 (0.002)   Loss 0.2250 (0.2864)   Prec@1 90.625 (90.107)   Prec@5 100.000 (99.752)   [2019-11-23 23:26:42]
  Epoch: [050][200/391]   Time 0.043 (0.050)   Data 0.000 (0.001)   Loss 0.2781 (0.2870)   Prec@1 94.531 (89.976)   Prec@5 99.219 (99.743)   [2019-11-23 23:26:47]
  Epoch: [050][300/391]   Time 0.043 (0.050)   Data 0.000 (0.001)   Loss 0.3668 (0.2906)   Prec@1 86.719 (89.781)   Prec@5 100.000 (99.733)   [2019-11-23 23:26:52]
  **Train** Prec@1 89.730 Prec@5 99.742 Error@1 10.270
  **Test** Prec@1 84.070 Prec@5 99.340 Error@1 15.930

==>>[2019-11-23 23:26:58] [Epoch=051/200] [Need: 00:53:12] [LR=0.0010][M=0.90] [Best : Accuracy=85.28, Error=14.72]
  Epoch: [051][000/391]   Time 0.251 (0.251)   Data 0.191 (0.191)   Loss 0.2215 (0.2215)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2019-11-23 23:26:59]
  Epoch: [051][100/391]   Time 0.042 (0.049)   Data 0.000 (0.002)   Loss 0.3506 (0.2857)   Prec@1 88.281 (89.921)   Prec@5 100.000 (99.737)   [2019-11-23 23:27:03]
  Epoch: [051][200/391]   Time 0.082 (0.050)   Data 0.000 (0.001)   Loss 0.1755 (0.2898)   Prec@1 93.750 (89.902)   Prec@5 100.000 (99.720)   [2019-11-23 23:27:09]
  Epoch: [051][300/391]   Time 0.043 (0.051)   Data 0.000 (0.001)   Loss 0.2468 (0.2911)   Prec@1 92.969 (89.826)   Prec@5 100.000 (99.722)   [2019-11-23 23:27:14]
  **Train** Prec@1 89.716 Prec@5 99.718 Error@1 10.284
  **Test** Prec@1 84.880 Prec@5 99.040 Error@1 15.120

==>>[2019-11-23 23:27:20] [Epoch=052/200] [Need: 00:52:50] [LR=0.0010][M=0.90] [Best : Accuracy=85.28, Error=14.72]
  Epoch: [052][000/391]   Time 0.275 (0.275)   Data 0.220 (0.220)   Loss 0.2811 (0.2811)   Prec@1 90.625 (90.625)   Prec@5 99.219 (99.219)   [2019-11-23 23:27:20]
  Epoch: [052][100/391]   Time 0.043 (0.052)   Data 0.000 (0.002)   Loss 0.2762 (0.2810)   Prec@1 91.406 (90.362)   Prec@5 99.219 (99.760)   [2019-11-23 23:27:25]
  Epoch: [052][200/391]   Time 0.074 (0.050)   Data 0.000 (0.001)   Loss 0.3236 (0.2863)   Prec@1 91.406 (90.209)   Prec@5 99.219 (99.724)   [2019-11-23 23:27:30]
  Epoch: [052][300/391]   Time 0.051 (0.049)   Data 0.000 (0.001)   Loss 0.2941 (0.2823)   Prec@1 90.625 (90.251)   Prec@5 100.000 (99.748)   [2019-11-23 23:27:35]
  **Train** Prec@1 90.200 Prec@5 99.756 Error@1 9.800
  **Test** Prec@1 85.370 Prec@5 99.360 Error@1 14.630
=> Obtain best accuracy, and update the best model

==>>[2019-11-23 23:27:41] [Epoch=053/200] [Need: 00:52:28] [LR=0.0010][M=0.90] [Best : Accuracy=85.37, Error=14.63]
  Epoch: [053][000/391]   Time 0.272 (0.272)   Data 0.219 (0.219)   Loss 0.1796 (0.1796)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2019-11-23 23:27:41]
